{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5213e104",
   "metadata": {},
   "source": [
    "# Image Analysis with Image Classification (VGG16)\n",
    "\n",
    "*This notebook provides a clear, endâ€‘toâ€‘end example of the **Image Data Analysis Roadmap** using a VGG16 model. Follow the sections in order: load data, inspect & clean, preprocess & augment, perform EDA, train VGG16, and evaluate (metrics + confusion matrix), then summarize insights.*\n",
    "\n",
    "> Tip: You can swap the dataset between **CIFARâ€‘10** (quick demo) and a **custom ImageFolder** (your own images) without changing the overall workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d92edc",
   "metadata": {},
   "source": [
    "## What weâ€™ll do\n",
    "1) Setup & Config â†’ seeds, device, knobs  \n",
    "2) Transforms â†’ preprocessing & augmentation  \n",
    "3) Load Data â†’ CIFARâ€‘10 or ImageFolder; splits  \n",
    "4) Inspect & Clean â†’ sample grid, class distribution  \n",
    "5) Aug Preview â†’ verify transforms visually  \n",
    "6) Model â†’ VGG16 (pretrained), replace head  \n",
    "7) Train & Validate â†’ curves, best checkpoint  \n",
    "8) Test & Explain â†’ accuracy, confusion matrix, Gradâ€‘CAM  \n",
    "9) Results & Report â†’ table template + notes  \n",
    "10) Summary â†’ next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f46e6",
   "metadata": {},
   "source": [
    "### 1) Setup & Config \n",
    "\n",
    "We centralize all experiment knobs here so you can tweak them without touching the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Config ====\n",
    "USE_CIFAR10 = True          # If False, use ImageFolder at CUSTOM_DATA_DIR\n",
    "CUSTOM_DATA_DIR = \"data\"    # Expect data/train, data/val, data/test\n",
    "\n",
    "IMG_SIZE = 224              # VGG16 expects 224x224\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "RANDOM_SEED = 42\n",
    "SAVE_BEST = True\n",
    "MODEL_OUT = \"vgg16_best.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc759c17",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- **Success criteria (define now):** e.g., *Val acc â‰¥ 80% by epoch 5; |trainâ€“val| â‰¤ 7pp*.\n",
    "- **Experiment grid:** (epochs Ã— LR) = {(3, 1eâ€‘3), (5, 1eâ€‘3), (5, 5eâ€‘4)}.\n",
    "- **Failure modes:** slow training â†’ freeze backbone / reduce IMG_SIZE; unstable loss â†’ lower LR / lighten jitter.\n",
    "- **Artifacts to log:** config dict, seed, package versions, GPU type, epoch time.\n",
    "  \n",
    "**Exercise:** Write your success criterion below and list 2 runs you plan to try.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f8a74",
   "metadata": {},
   "source": [
    "### 2) Imports, Device & Reproducibility\n",
    "\n",
    "We import PyTorch/torchvision, detect GPU, and fix seeds for repeatability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140028d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, math, itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Seeds\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1996c16",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- With seeds fixed and val transforms deterministic, results should be similar across runs.\n",
    "- If training on CPU: reduce epochs/batch size for time.\n",
    "  \n",
    "**Exercise:** Note the device printed; if CPU, adjust `EPOCHS` to a lower number you can afford.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feec9c4",
   "metadata": {},
   "source": [
    "### 3) Data Transforms - *preprocessing & augmentation*\n",
    "\n",
    "Training uses light augmentation; val/test are deterministic. We match ImageNet normalization for VGG16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513aa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc278d",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics - Transforms**\n",
    "- **Normalization check:** Using ImageNet stats prevents distribution shift to VGG16.\n",
    "- **Aug strength:** If trainâ‰«val â†’ stronger aug; if both low/noisy â†’ dial back aug or LR.\n",
    "  \n",
    "**Exercise:** Add `transforms.RandomRotation(10)` to `train_tfms` and later compare validation accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d6352",
   "metadata": {},
   "source": [
    "### 4) ðŸ“‚ Load Data - *CIFARâ€‘10 or ImageFolder*\n",
    "\n",
    "Use CIFARâ€‘10 for baseline; switch to your foldered dataset when ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3096a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CIFAR10:\n",
    "    trainset_full = datasets.CIFAR10(root=\"data_cifar10\", train=True, download=True, transform=train_tfms)\n",
    "    testset = datasets.CIFAR10(root=\"data_cifar10\", train=False, download=True, transform=val_tfms)\n",
    "    num_classes = 10\n",
    "    class_names = trainset_full.classes\n",
    "    # Validation split\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(trainset_full) * val_ratio)\n",
    "    train_size = len(trainset_full) - val_size\n",
    "    trainset, valset = random_split(trainset_full, [train_size, val_size], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "else:\n",
    "    train_dir = os.path.join(CUSTOM_DATA_DIR, \"train\")\n",
    "    val_dir   = os.path.join(CUSTOM_DATA_DIR, \"val\")\n",
    "    test_dir  = os.path.join(CUSTOM_DATA_DIR, \"test\")\n",
    "\n",
    "    trainset = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "    valset   = datasets.ImageFolder(val_dir, transform=val_tfms)\n",
    "    testset  = datasets.ImageFolder(test_dir, transform=val_tfms)\n",
    "\n",
    "    class_names = trainset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Num classes:\", num_classes)\n",
    "print(\"Train/Val/Test sizes:\", len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8619a",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics - Data Integrity**\n",
    "- Quickly scan class names for typos; ensure splits are sensible.\n",
    "- Avoid leakage: same/similar images must not appear across different splits.\n",
    "  \n",
    "**Exercise:** If using ImageFolder, list any classes with very few samples and propose remedies (augmentation / more data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dc599",
   "metadata": {},
   "source": [
    "### 5) ðŸšš DataLoaders - *batching & shuffling*\n",
    "\n",
    "We prepare loaders with shuffling for train and deterministic order for val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963929f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "valloader   = DataLoader(valset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "testloader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cfcca",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics **\n",
    "- If I/O is slow, increase `NUM_WORKERS`; if unstable, reduce it.\n",
    "- `pin_memory=True` helps on GPU.\n",
    "\n",
    "**Exercise:** Time one epoch later and compute images/sec. Record it in the results table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d813d",
   "metadata": {},
   "source": [
    "### 6) Inspect Samples & Class Distribution - *quick EDA*\n",
    "\n",
    "We visualize a grid of samples and check class counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_grid(images, labels, classes, n=16):\n",
    "    images = images[:n].cpu()\n",
    "    labels = labels[:n].cpu()\n",
    "    grid = int(math.sqrt(n))\n",
    "    fig, axes = plt.subplots(grid, grid, figsize=(8,8))\n",
    "    idx = 0\n",
    "    for r in range(grid):\n",
    "        for c in range(grid):\n",
    "            img = images[idx].permute(1,2,0).numpy()\n",
    "            img = (img * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])).clip(0,1)\n",
    "            axes[r,c].imshow(img); axes[r,c].axis('off')\n",
    "            axes[r,c].set_title(classes[labels[idx].item()], fontsize=8)\n",
    "            idx += 1\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "batch_imgs, batch_labels = next(iter(trainloader))\n",
    "imshow_grid(batch_imgs, batch_labels, class_names, n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_class(dset, class_names):\n",
    "    counts = {c:0 for c in class_names}\n",
    "    if hasattr(dset, 'dataset') and hasattr(dset, 'indices'):\n",
    "        base = dset.dataset\n",
    "        for idx in dset.indices:\n",
    "            if isinstance(base, datasets.CIFAR10):\n",
    "                _, y = base[idx]\n",
    "                counts[class_names[y]] += 1\n",
    "            elif isinstance(base, datasets.ImageFolder):\n",
    "                y = base.targets[idx]\n",
    "                counts[class_names[y]] += 1\n",
    "    else:\n",
    "        if isinstance(dset, datasets.CIFAR10):\n",
    "            for i in range(len(dset)):\n",
    "                _, y = dset[i]\n",
    "                counts[class_names[y]] += 1\n",
    "        elif isinstance(dset, datasets.ImageFolder):\n",
    "            for y in dset.targets:\n",
    "                counts[class_names[y]] += 1\n",
    "    return counts\n",
    "\n",
    "train_counts = count_by_class(trainset, class_names)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(train_counts.keys(), train_counts.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Train Class Distribution\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8347201",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics - EDA**\n",
    "- Look for label noise, odd crops, or colour issues.\n",
    "- Identify underrepresented classes (<5% of samples).\n",
    "\n",
    "**Exercise:** Write two observations from the grid and one class you expect to be hardest (and why).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da1d64",
   "metadata": {},
   "source": [
    "### 7) Augmentation Preview\n",
    "\n",
    "We visualize augmented images to ensure transforms preserve semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae22624",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def show_aug_examples(dataset, n=8):\n",
    "    fig, axes = plt.subplots(1, n, figsize=(2*n, 2))\n",
    "    for i in range(n):\n",
    "        x, y = dataset[i]\n",
    "        x_np = x.permute(1,2,0).numpy()\n",
    "        x_np = (x_np * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406]))\n",
    "        x_np = np.clip(x_np, 0, 1)\n",
    "        axes[i].imshow(x_np); axes[i].axis('off')\n",
    "    plt.suptitle(\"Augmentation Preview\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "base_trainset = trainset.dataset if hasattr(trainset, 'dataset') else trainset\n",
    "show_aug_examples(base_trainset, n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a621a",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Human should still label them correctly.\n",
    "- Balance diversity and realism.\n",
    "\n",
    "**Exercise:** Increase jitter strength and note any harmful artifacts you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222ae0d",
   "metadata": {},
   "source": [
    "### 8) Build VGG16 (Pretrained) - *transfer learning*\n",
    "\n",
    "We replace the classifier head; start by freezing the feature extractor for a fast baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7638df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = vgg.classifier[-1].in_features\n",
    "vgg.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "for p in vgg.features.parameters():\n",
    "    p.requires_grad = False  # unfreeze later for better accuracy\n",
    "\n",
    "vgg = vgg.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vgg.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
    "\n",
    "print(vgg.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda00b2",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics - Transfer Strategy**\n",
    "- **Baseline:** headâ€‘only training is your control.\n",
    "- **Upgrade:** unfreeze last conv block later with a **smaller LR**.\n",
    "\n",
    "**Exercise:** Note the in_features printed and confirm the head output matches `num_classes`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5bc74",
   "metadata": {},
   "source": [
    "### 9) Training & Evaluation Utilities\n",
    "\n",
    "We implement clean functions for one epoch of training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c35f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == targets).sum().item()\n",
    "    return correct / targets.size(0)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        running_acc  += accuracy_from_logits(logits, y) * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return running_loss / n, running_acc / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        running_acc  += accuracy_from_logits(logits, y) * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return running_loss / n, running_acc / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75178dd3",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics - Metrics**\n",
    "- Accuracy is fine for balanced classes; for imbalance, add macroâ€‘F1.\n",
    "  \n",
    "**Exercise:** Extend `evaluate` to also compute macroâ€‘F1 using `sklearn.metrics.f1_score` and compare to accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23dda65",
   "metadata": {},
   "source": [
    "### 10) Train the Model\n",
    "\n",
    "We run multiple epochs, adjust LR when val stalls, save best, and plot curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9790736",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = -1.0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start = time.time()\n",
    "    tl, ta = train_one_epoch(vgg, trainloader, optimizer, criterion)\n",
    "    vl, va = evaluate(vgg, valloader, criterion)\n",
    "    scheduler.step(va)\n",
    "    history[\"train_loss\"].append(tl); history[\"train_acc\"].append(ta)\n",
    "    history[\"val_loss\"].append(vl);   history[\"val_acc\"].append(va)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Epoch {epoch:02d} | Train loss {tl:.4f} acc {ta:.4f} | Val loss {vl:.4f} acc {va:.4f} | {elapsed:.1f}s\")\n",
    "    if SAVE_BEST and va > best_val_acc:\n",
    "        best_val_acc = va\n",
    "        torch.save(vgg.state_dict(), MODEL_OUT)\n",
    "        print(f\"  â†³ Saved new best to {MODEL_OUT}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796232c",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- **Overfitting:** trainâ†‘ valâ†“ â†’ add augmentation/regularization or unfreeze gradually with lower LR.\n",
    "- **Underfitting:** both low â†’ train longer, lower LR, or unfreeze more layers.\n",
    "  \n",
    "**Exercise:** Note epoch of best val accuracy and the learningâ€‘curve shape in your results table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccba2c3",
   "metadata": {},
   "source": [
    "### 11) Test Evaluation & Confusion Matrix - *generalization check*\n",
    "\n",
    "We compute test accuracy, confusion matrix, and perâ€‘class report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755300eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_all(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(y.numpy().tolist())\n",
    "    return np.array(all_preds), np.array(all_targets)\n",
    "\n",
    "preds, targets = predict_all(vgg, testloader)\n",
    "test_acc = (preds == targets).mean()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(targets, preds, labels=list(range(num_classes)))\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(num_classes)\n",
    "plt.xticks(tick_marks, class_names, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, class_names)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(targets, preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d021f",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Identify bottomâ€‘3 classes by recall; inspect 10 misclassifications each to find patterns.\n",
    "- Consider classâ€‘balanced sampling or targeted augmentation.\n",
    "\n",
    "**Exercise:** Write two concrete actions to improve the weakest classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65934eea",
   "metadata": {},
   "source": [
    "### 12) Gradâ€‘CAM *qualitative explainability*\n",
    "\n",
    "We overlay Gradâ€‘CAM to visualize which regions influenced predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# last conv layer in VGG16\n",
    "target_layer = vgg.features[28]\n",
    "_acts = None; _grads = None\n",
    "\n",
    "def _forward_hook(module, inp, out):\n",
    "    global _acts; _acts = out.detach()\n",
    "\n",
    "def _backward_hook(module, grad_in, grad_out):\n",
    "    global _grads; _grads = grad_out[0].detach()\n",
    "\n",
    "fh = target_layer.register_forward_hook(_forward_hook)\n",
    "bh = target_layer.register_full_backward_hook(_backward_hook)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _denorm(img_t):\n",
    "    img = img_t.clone().cpu().permute(1,2,0).numpy()\n",
    "    img = (img * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406]))\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "def grad_cam(model, x, class_idx=None):\n",
    "    global _acts, _grads\n",
    "    _acts, _grads = None, None\n",
    "    model.eval()\n",
    "    x = x.to(device).requires_grad_(True)\n",
    "    logits = model(x)\n",
    "    if class_idx is None:\n",
    "        class_idx = logits.argmax(1).item()\n",
    "    score = logits[0, class_idx]\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward(retain_graph=True)\n",
    "    acts = _acts[0]; grads = _grads\n",
    "    weights = grads.mean(dim=(1,2))\n",
    "    cam = (weights[:, None, None] * acts).sum(dim=0)\n",
    "    cam = torch.clamp(cam, min=0)\n",
    "    cam -= cam.min()\n",
    "    if cam.max() > 0: cam /= cam.max()\n",
    "    cam = torch.nn.functional.interpolate(cam[None,None], size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)[0,0]\n",
    "    return cam.detach().cpu().numpy(), class_idx\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "img = images[0].unsqueeze(0)\n",
    "true_label = labels[0].item()\n",
    "\n",
    "heatmap, pred_idx = grad_cam(vgg, img)\n",
    "base = _denorm(images[0])\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1); plt.title(f\"Image\\nTrue: {class_names[true_label]} | Pred: {class_names[pred_idx]}\"); plt.imshow(base); plt.axis('off')\n",
    "plt.subplot(1,2,2); plt.title(\"Grad-CAM Overlay\"); plt.imshow(base); plt.imshow(heatmap, alpha=0.35); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fh.remove(); bh.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955d96c",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Attention should land on the object of interest; if not, revisit preprocessing/augmentation.\n",
    "- Compare Gradâ€‘CAM on correct vs incorrect predictions.\n",
    "\n",
    "**Exercise:** Run Gradâ€‘CAM on a misclassified example and describe what the heatmap suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd42a68",
   "metadata": {},
   "source": [
    "### 13) Results & Report\n",
    "\n",
    "Use this table to keep a clean record of your runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_cols = [\"run_id\",\"epochs\",\"lr\",\"freeze_backbone\",\"val_acc\",\"test_acc\",\"epoch_time_s\",\"notes\"]\n",
    "results = pd.DataFrame(columns=results_cols)\n",
    "results_path = \"results_log.csv\"\n",
    "results.to_csv(results_path, index=False)\n",
    "print(\"Created results template at\", results_path)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abc470",
   "metadata": {},
   "source": [
    "**How to use:**\n",
    "- After each run, append a new row with your configuration and metrics.\n",
    "- Keep notes on augmentations and any issues encountered.\n",
    "  \n",
    "**Exercise:** After training, add a row to the table with your best run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2f4a3",
   "metadata": {},
   "source": [
    "## âœ… Summary & Next Steps\n",
    "\n",
    "**You completed:** config â†’ transforms â†’ data loading â†’ EDA â†’ augmentation preview â†’ VGG16 setup â†’ training â†’ evaluation â†’ Gradâ€‘CAM â†’ results logging.\n",
    "\n",
    "**Next:** \n",
    "- Unfreeze layers and sweep LR for backbone vs head (discriminative LRs).  \n",
    "- Try **CutMix/MixUp** and classâ€‘balanced sampling for imbalance.  \n",
    "- Compare with **ResNetâ€‘50** or **ViT** and document findings.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
