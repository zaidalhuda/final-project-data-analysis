{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47613c14",
   "metadata": {},
   "source": [
    "# Image Data Analysis Roadmap\n",
    "*This notebook provides a structured roadmap for analyzing image datasets. It outlines each step from setup, loading, cleaning, and preprocessing to exploratory analysis, augmentation, and evaluation. Students can follow these stages to prepare and analyze image data for tasks such as classification, detection, and segmentation.*\n",
    "\n",
    "## 1) Setup & Imports\n",
    "\n",
    "**Core**\n",
    "\n",
    "* `numpy`, `pandas`, `matplotlib.pyplot`, `PIL.Image`, `cv2`\n",
    "* For deep learning: `torch`, `torchvision` (or `tensorflow`, `keras`)\n",
    "* Augmentations: `albumentations`, `torchvision.transforms`\n",
    "* Hashing/duplicates: `imagehash`, `PIL`\n",
    "* EXIF: `piexif`, `PIL.ExifTags`\n",
    "* Format/IO at scale: `tqdm`, `pathlib`, `pyyaml`, `json`, `webdataset`/`tfrecord`\n",
    "\n",
    "**Reproducibility**\n",
    "\n",
    "* Set seeds (`random`, `numpy`, framework seeds)\n",
    "* Log package versions (`pip freeze`), GPU info\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Load Image Data\n",
    "\n",
    "**Folder structures**\n",
    "\n",
    "* Classification: `data/{train,val,test}/{class_name}/*.jpg`\n",
    "* Detection/Segmentation: images + labels (COCO/YOLO/Pascal VOC; masks as PNG)\n",
    "* External sources: Kaggle ZIPs, Google Drive, S3\n",
    "\n",
    "**Methods**\n",
    "\n",
    "* Build a manifest DataFrame: file path, split, class/labels, width, height, format, checksum/hash\n",
    "* Validate paths & count per split/class\n",
    "* For detection/segmentation: parse label files; verify image–annotation alignment\n",
    "\n",
    "**Quick checks**\n",
    "\n",
    "* Can every listed image be opened?\n",
    "* Any zero-byte or unreadable files?\n",
    "\n",
    "**Task-specific Data Loaders (must build; no code here)**\n",
    "\n",
    "* **Classification:** a loader that returns `(image, class_label)`\n",
    "* **Detection:** a loader that returns `(image, bounding_boxes, class_labels)` and is **bbox/mask-aware** for transforms\n",
    "* **Segmentation:** a loader that returns `(image, mask)` with **aligned** image–mask sizes\n",
    "\n",
    "**Loader requirements**\n",
    "\n",
    "* Apply consistent preprocessing (resize, normalization, augmentations)\n",
    "* Cleanly support `train/val/test` splits\n",
    "* Be modular so datasets can be swapped without changing training logic\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Inspect Image Data\n",
    "\n",
    "**Image metadata**\n",
    "\n",
    "* Resolution stats (min/median/max, aspect ratios)\n",
    "* Color mode (`L`, `RGB`, `RGBA`); color profile/ICC\n",
    "* EXIF presence (orientation, datetime)\n",
    "\n",
    "**Dataset composition**\n",
    "\n",
    "* Per-class counts & imbalance ratios\n",
    "* Split sizes & class balance **per split**\n",
    "* Annotation coverage (avg boxes per image, mask area %, category frequency)\n",
    "\n",
    "**Visual peeks**\n",
    "\n",
    "* Random grids per class\n",
    "* Few annotated examples (boxes/masks overlay)\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Data Cleaning (Quality & Integrity)\n",
    "\n",
    "**File & format**\n",
    "\n",
    "* Remove corrupted/unreadable files (`PIL`/`cv2` try/except)\n",
    "* Standardize formats (e.g., all `.jpg` or `.png`)\n",
    "* Fix EXIF orientation and strip problematic EXIF if needed\n",
    "\n",
    "**Near-duplicates / leakage**\n",
    "\n",
    "* Perceptual hashing (`imagehash.average_hash/phash/dhash`) to find duplicates\n",
    "* Cluster by hash distance to remove or keep one representative\n",
    "* **Split leakage check**: ensure near-duplicates do not cross train/val/test\n",
    "\n",
    "**Label QA**\n",
    "\n",
    "* Class spelling/ontology normalization (e.g., `cat` vs `cats`)\n",
    "* Out-of-range boxes, negative coords, boxes outside image bounds → fix or drop\n",
    "* Masks: empty masks, mismatched sizes, non-binary pixels → fix\n",
    "* Inter-annotator agreement sample (Cohen’s κ for category labels)\n",
    "\n",
    "**Image quality**\n",
    "\n",
    "* Blur detection (Laplacian variance); flag too-blurry images\n",
    "* Noise/artifacts (hot/dead pixels, compression blocks) → consider denoising\n",
    "* Exposure issues: histogram clipping; extreme brightness/contrast\n",
    "* Broken transparency (unexpected alpha channel)\n",
    "\n",
    "**Ethics/PII**\n",
    "\n",
    "* Faces/plates—blur or remove if policy requires\n",
    "* Copyright/source audit trail\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Splitting Strategy (Prevent Leakage)\n",
    "\n",
    "**Methods**\n",
    "\n",
    "* **Stratified** split by class for classification\n",
    "* **Group-aware** split (patient ID, video ID, scene/location) to avoid correlated leakage\n",
    "* **Time-aware** split if the task is temporal\n",
    "* Ensure **aspect-ratio distribution** and **quality distributions** are similar across splits\n",
    "\n",
    "**Validation design**\n",
    "\n",
    "* K-fold or GroupKFold for small datasets\n",
    "* Fixed hold-out + cross-val on train when tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Preprocessing (Standardization)\n",
    "\n",
    "**Geometry**\n",
    "\n",
    "* Resize with preserved content:\n",
    "  * Classification: fixed size (e.g., 224×224, 299×299) with center/letterbox padding\n",
    "  * Detection: keep aspect ratio + letterbox\n",
    "  * Segmentation: consistent size for image & mask\n",
    "* Optional cropping (center, face/object-aware, or content-aware)\n",
    "\n",
    "**Photometric**\n",
    "\n",
    "* Normalize to [0,1] or [-1,1]; mean/std normalization (dataset-specific or ImageNet stats)\n",
    "* Color space conversion (`BGR↔RGB`, `RGB↔Lab/HSV`)\n",
    "* Histogram Equalization / CLAHE (grayscale or per-channel; careful for color shifts)\n",
    "\n",
    "**Denoise / Deblur (if needed)**\n",
    "\n",
    "* Median/Bilateral filter (small, conservative)\n",
    "* Non-local means; gentle settings\n",
    "* Avoid heavy deblurring unless justified (can distort labels)\n",
    "\n",
    "**Consistency**\n",
    "\n",
    "* Ensure preprocessing mirrors inference (document transforms)\n",
    "* Cache preprocessed outputs if I/O bound\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Data Augmentation (Task-Specific)\n",
    "\n",
    "**Geometric**\n",
    "\n",
    "* Flip (H/V), small rotations, scale, shear, translate, perspective\n",
    "* For detection: use bbox/mask-aware transforms (Albumentations with bbox/mask params)\n",
    "* Maintain label validity after transforms\n",
    "\n",
    "**Photometric**\n",
    "\n",
    "* Brightness/contrast, gamma, hue/saturation, grayscale, Gaussian noise\n",
    "* JPEG compression artifacts for robustness\n",
    "\n",
    "**Cut-style / Mix-style**\n",
    "\n",
    "* Cutout, Random Erasing\n",
    "* MixUp, CutMix (classification)\n",
    "* Mosaic, MixUp (detection, e.g., YOLO-style)\n",
    "\n",
    "**Policies**\n",
    "\n",
    "* RandAugment, AutoAugment (classification)\n",
    "* Probability scheduling (lighter aug early or later)\n",
    "* Keep an **augment vs. no-augment** ablation to justify choices\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Feature Engineering (Optional, Classical)\n",
    "\n",
    "**Classical descriptors**\n",
    "\n",
    "* Color histograms, Haralick textures (GLCM), LBP\n",
    "* SIFT/ORB (if allowed) → Bag-of-Visual-Words / VLAD\n",
    "\n",
    "**Deep features**\n",
    "\n",
    "* Extract embeddings from pretrained CNN backbones (ResNet, ViT) for:\n",
    "  * EDA (t-SNE/UMAP)\n",
    "  * Simple classifiers (Logistic/Linear SVM) as baselines\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Distribution plots**\n",
    "\n",
    "* Class counts (bar), per-class sample grids\n",
    "* Image width/height & aspect ratio histograms\n",
    "* Sharpness/blur score distribution; brightness/contrast distributions\n",
    "* Channel means/stds per split\n",
    "\n",
    "**Qualitative panels**\n",
    "\n",
    "* Before/after preprocessing and augmentation comparisons\n",
    "* Outlier gallery: extreme sizes, extreme brightness, heavy blur\n",
    "\n",
    "**Embeddings**\n",
    "\n",
    "* t-SNE/UMAP of deep features to see cluster separability & mislabeled points\n",
    "\n",
    "**Annotation EDA**\n",
    "\n",
    "* Detection: box size distribution, aspect ratios, per-image box counts\n",
    "* Segmentation: mask area %, boundary complexity, class co-occurrence\n",
    "\n",
    "**Leakage & imbalance**\n",
    "\n",
    "* Near-duplicate heatmaps across splits\n",
    "* Imbalance visuals; decide on weighted loss, focal loss, or sampling\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Baselines & Checks (Tiny Models)\n",
    "\n",
    "**Baselines**\n",
    "\n",
    "* Classical: deep-feature + logistic regression\n",
    "* DL quick baseline: small CNN or pretrained head (frozen backbone)\n",
    "\n",
    "**Sanity**\n",
    "\n",
    "* **Label shuffle test** (should train poorly)\n",
    "* Train on a **small subset** (should overfit)\n",
    "* Quick Grad-CAM on a few images to check the model looks at the object, not corners/watermarks\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Documentation & Dataset Card\n",
    "\n",
    "**Include**\n",
    "\n",
    "* Data source(s), licenses, collection dates\n",
    "* Preprocessing & augmentation summary\n",
    "* Known limitations/biases & PII handling\n",
    "* Splits rationale and leakage checks\n",
    "* Versioning: dataset vX.Y with changelog\n",
    "* Repro steps: scripts/commands to rebuild\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Packaging for Training\n",
    "\n",
    "**Pipelines**\n",
    "\n",
    "* PyTorch `Dataset`/`DataLoader` or TF `tf.data`\n",
    "* COCO/YOLO/VOC converters (one canonical format)\n",
    "* Sharding: TFRecords or WebDataset (tar shards) for speed\n",
    "* Caching: memory-mapped images (`opencv + turbojpeg`), on-the-fly decode\n",
    "\n",
    "**Performance**\n",
    "\n",
    "* Worker count, pinned memory, prefetching\n",
    "* Mixed precision readiness (if training later)\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Readiness Checklist (Go/No-Go)\n",
    "\n",
    "* [ ] All images open; formats standardized\n",
    "* [ ] No cross-split near-duplicates\n",
    "* [ ] Labels validated (boxes/masks sane)\n",
    "* [ ] Splits stratified/group-aware and balanced\n",
    "* [ ] Preprocessing & augmentation defined and reproducible\n",
    "* [ ] EDA done; issues addressed or documented\n",
    "* [ ] Baseline sanity checks passed\n",
    "* [ ] Dataset card complete; license clear\n",
    "\n",
    "---\n",
    "\n",
    "## 14) Task-Specific Metrics (for when you evaluate)\n",
    "\n",
    "* **Classification**: Accuracy, Precision/Recall/F1 (macro), AUROC, confusion matrix\n",
    "* **Detection**: mAP@[.5:.95], per-class AP, AR vs. #detections\n",
    "* **Segmentation**: mIoU, Dice/F1, boundary F-score\n",
    "* **Image quality tasks**: PSNR, SSIM (if relevant)\n",
    "* **Robustness**: performance under augment-like perturbations\n",
    "\n",
    "---\n",
    "\n",
    "### Notes for Students\n",
    "\n",
    "* **Start simple**: get a clean baseline before heavy augmentation.\n",
    "* **Measure, don’t guess**: every cleaning step should show a measurable benefit or clear risk reduction (e.g., leakage).\n",
    "* **Document everything**: what you changed and why—future you will thank present you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9a507",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
