{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b15f74a",
   "metadata": {},
   "source": [
    "# Image Analysis with Video Object Detection \n",
    "\n",
    "This notebook is a **clear, end‑to‑end example** of the *Video Data Analysis Roadmap* applied to **object detection** on videos.\n",
    "It uses **OpenCV** to decode frames and **torchvision’s Faster R‑CNN (COCO pretrained)** to detect objects per frame.\n",
    "\n",
    "**You’ll find:** explanations before each code cell, analysis & diagnostics after each step, exercises, and a results table template.\n",
    "\n",
    "> Tip: Start with a **short clip (≤30s)** while you iterate. Later, scale up and measure throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22123d5b",
   "metadata": {},
   "source": [
    "## What we’ll do\n",
    "1) Setup & Config → paths, sampling, thresholds  \n",
    "2) Imports, Device & Reproducibility  \n",
    "3) Video Probing → fps, duration, resolution, codec  \n",
    "4) Frame Sampling → decode frames at stride S, manifest  \n",
    "5) Transforms → preprocessing to tensors  \n",
    "6) Model → Faster R‑CNN (COCO), score threshold  \n",
    "7) Inference on Frames → batch detection, timings  \n",
    "8) Visualization → draw boxes/labels; save annotated video  \n",
    "9) Results & Diagnostics → summary stats and guidance  \n",
    "10) Summary & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2be90",
   "metadata": {},
   "source": [
    "### 1) Setup & Config \n",
    "All experiment knobs live here. Edit paths, sampling, and thresholds without touching later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = 'sample.mp4'\n",
    "OUT_VIDEO  = 'annotated_output.mp4'\n",
    "FRAME_STRIDE = 5\n",
    "MAX_FRAMES   = 500\n",
    "BATCH_SIZE   = 4\n",
    "SCORE_THRESH = 0.5\n",
    "NMS_IOU      = 0.5\n",
    "RANDOM_SEED  = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d911a",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Sampling trade‑off: larger stride → faster but may miss events.\n",
    "- Score threshold: higher → precision↑, recall↓.\n",
    "- Batch size: limited by GPU memory; detectors are heavy.\n",
    "\n",
    "**Exercise:** Define a success criterion and pick two (stride, threshold) settings to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431edd48",
   "metadata": {},
   "source": [
    "### 2) Imports, Device & Reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, random, json, pathlib\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
    "    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
    "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog',\n",
    "    'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb7bc7",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Expect cuda on GPU machines; CPU works but slower.\n",
    "- Determinism slightly reduces speed; acceptable for teaching.\n",
    "\n",
    "**Exercise:** If on CPU, set MAX_FRAMES=200 and BATCH_SIZE=2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3630238",
   "metadata": {},
   "source": [
    "### 3) Probe Video - *fps, duration, resolution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(VIDEO_PATH), f'Video not found: {VIDEO_PATH}'\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), f'Could not open video: {VIDEO_PATH}'\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 0\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 0\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 0\n",
    "duration_s = num_frames / fps if fps > 0 else 0.0\n",
    "print(f'FPS: {fps:.2f} | Frames: {num_frames} | Resolution: {width}x{height} | Duration: {duration_s:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00a370",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Validate fps/resolution; odd values may signal VFR or corruption.\n",
    "\n",
    "**Exercise:** Estimate #processed frames with your stride and max cap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dfa73",
   "metadata": {},
   "source": [
    "### 4) Frame Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(cap, stride, max_frames):\n",
    "    frames = []\n",
    "    idx = 0; kept = 0\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    while True:\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok: break\n",
    "        if idx % stride == 0:\n",
    "            frames.append(frame_bgr)\n",
    "            kept += 1\n",
    "            if kept >= max_frames:\n",
    "                break\n",
    "        idx += 1\n",
    "    return frames\n",
    "\n",
    "frames_bgr = sample_frames(cap, FRAME_STRIDE, MAX_FRAMES)\n",
    "cap.release()\n",
    "print('Kept', len(frames_bgr), 'frames; example shape:', frames_bgr[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e16677",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Keep frame count manageable first; scale up later.\n",
    "\n",
    "**Exercise:** Try stride=2 and stride=8; record kept frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b8d01",
   "metadata": {},
   "source": [
    "### 5) Preprocessing for Detector - *BGR→RGB, to tensor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fae998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def bgr_to_rgb(img_bgr):\n",
    "    import cv2\n",
    "    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "rgb0 = bgr_to_rgb(frames_bgr[0])\n",
    "print('RGB sample shape:', rgb0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79d4a7",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Ensure RGB ordering; wrong channels harm accuracy.\n",
    "\n",
    "**Exercise:** Visualize BGR vs RGB with imshow to see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a78e40",
   "metadata": {},
   "source": [
    "### 6) Model - Faster R‑CNN (COCO pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    ")\n",
    "model = model.to(device).eval()\n",
    "print('Model ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e9330",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Trained on COCO (80 classes); domain shift may reduce accuracy.\n",
    "\n",
    "**Exercise:** Name 3 classes you expect here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ad986",
   "metadata": {},
   "source": [
    "### 7) Inference on Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "\n",
    "def run_detector_on_frames(frames_bgr, batch_size=4, score_thresh=0.5):\n",
    "    detections = []\n",
    "    t0 = time.time()\n",
    "    tensors = [to_tensor(bgr_to_rgb(f)) for f in frames_bgr]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tensors), batch_size):\n",
    "            batch = [t.to(device) for t in tensors[i:i+batch_size]]\n",
    "            outputs = model(batch)\n",
    "            for out in outputs:\n",
    "                keep = out['scores'] >= score_thresh\n",
    "                detections.append({\n",
    "                    'boxes': out['boxes'][keep].detach().cpu().numpy(),\n",
    "                    'labels': out['labels'][keep].detach().cpu().numpy(),\n",
    "                    'scores': out['scores'][keep].detach().cpu().numpy(),\n",
    "                })\n",
    "    elapsed = time.time() - t0\n",
    "    return detections, elapsed\n",
    "\n",
    "dets, infer_time = run_detector_on_frames(frames_bgr, BATCH_SIZE, SCORE_THRESH)\n",
    "fps_proc = len(frames_bgr)/infer_time if infer_time>0 else 0\n",
    "print(f'Inference: {infer_time:.2f}s, {fps_proc:.2f} FPS processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dfea8",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Track processed FPS; adjust stride/batch for speed.\n",
    "- Tune threshold for precision/recall trade‑off.\n",
    "\n",
    "**Exercise:** Try thresholds 0.3 and 0.7; compare outputs qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72198f7d",
   "metadata": {},
   "source": [
    "### 8) Visualization & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def draw_boxes_on_frame(frame_bgr, boxes, labels, scores):\n",
    "    frame = frame_bgr.copy()\n",
    "    for (x1,y1,x2,y2), lab, sc in zip(boxes, labels, scores):\n",
    "        color = (0,255,0)\n",
    "        cv2.rectangle(frame, (int(x1),int(y1)), (int(x2),int(y2)), color, 2)\n",
    "        name = COCO_INSTANCE_CATEGORY_NAMES[int(lab)] if int(lab) < len(COCO_INSTANCE_CATEGORY_NAMES) else str(int(lab))\n",
    "        cv2.putText(frame, f'{name}:{sc:.2f}', (int(x1), max(0,int(y1)-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "# writer\n",
    "cap2 = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps2 = cap2.get(cv2.CAP_PROP_FPS) or 24\n",
    "w2  = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH)) or 640\n",
    "h2  = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 360\n",
    "cap2.release()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "writer = cv2.VideoWriter(OUT_VIDEO, fourcc, fps2, (w2, h2))\n",
    "\n",
    "for f_bgr, det in zip(frames_bgr, dets):\n",
    "    frame_anno = draw_boxes_on_frame(f_bgr, det['boxes'], det['labels'], det['scores'])\n",
    "    if frame_anno.shape[1] != w2 or frame_anno.shape[0] != h2:\n",
    "        frame_anno = cv2.resize(frame_anno, (w2, h2))\n",
    "    writer.write(frame_anno)\n",
    "writer.release()\n",
    "print('Annotated video saved to:', OUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452f092",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Confirm boxes align with objects; misalignment suggests preprocessing/resize issues.\n",
    "\n",
    "**Exercise:** Modify drawing to only show `person` detections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa57808",
   "metadata": {},
   "source": [
    "### 9) Results & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cls_counter = Counter()\n",
    "boxes_per_frame = []\n",
    "for det in dets:\n",
    "    labs = det['labels'].tolist()\n",
    "    boxes_per_frame.append(len(labs))\n",
    "    for l in labs:\n",
    "        name = COCO_INSTANCE_CATEGORY_NAMES[int(l)] if int(l) < len(COCO_INSTANCE_CATEGORY_NAMES) else str(int(l))\n",
    "        cls_counter[name] += 1\n",
    "\n",
    "print('Total frames:', len(dets))\n",
    "print('Total detections:', sum(boxes_per_frame))\n",
    "print('Top classes:', cls_counter.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(6,3)); plt.plot(boxes_per_frame); plt.title('Detections per Frame'); plt.xlabel('Frame idx'); plt.ylabel('#'); plt.tight_layout(); plt.show()\n",
    "\n",
    "names = [k for k,_ in cls_counter.most_common(10)]\n",
    "vals  = [v for _,v in cls_counter.most_common(10)]\n",
    "plt.figure(figsize=(6,3)); plt.bar(names, vals); plt.xticks(rotation=45, ha='right'); plt.title('Top Classes'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd18ec",
   "metadata": {},
   "source": [
    "**Analysis & Diagnostics**\n",
    "- Spikes → crowded scenes or false positives; dips → empty or simple frames.\n",
    "- Missing expected classes? Lower threshold or consider domain shift.\n",
    "\n",
    "**Exercise:** Write two observations and one hypothesis about spikes/dips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567cf592",
   "metadata": {},
   "source": [
    "### 10) Results & Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_cols = ['run_id','frame_stride','batch_size','score_thresh','n_frames','proc_fps','top_classes','notes']\n",
    "results = pd.DataFrame(columns=results_cols)\n",
    "results_path = 'video_det_results.csv'\n",
    "results.to_csv(results_path, index=False)\n",
    "print('Created results template at', results_path)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b4d0c",
   "metadata": {},
   "source": [
    "**How to use**\n",
    "- Add a row after each run with parameters and key summaries.\n",
    "- Paste the `Top classes` output into the table.\n",
    "\n",
    "**Exercise:** Add two runs with different thresholds or strides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d52452",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "**You completed:** probing → sampling → preprocessing → detector → batched inference → annotated export → diagnostics → logging.\n",
    "\n",
    "**Next:**\n",
    "- Add **tracking** (Deep SORT / ByteTrack) to link detections across frames.\n",
    "- Try other detectors (YOLOv8/RT‑DETR) or finetune on your dataset.\n",
    "- If you have annotations, compute **mAP**; otherwise, build a qualitative review pipeline."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
